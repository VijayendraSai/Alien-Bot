{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_CYPJObjhJ9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "from scipy import stats\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from google.colab import drive\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"/content/drive/MyDrive/AI-Project-3/model1_data_final.xlsx\")\n",
        "\n",
        "ship = data['ship']\n",
        "alien_prob = data['beliefNetworkAlien']\n",
        "crew_prob = data['beliefNetworkCrew']\n",
        "bot = data['bot_cell']\n",
        "alien_beep = data[\"isBeepAlien\"]\n",
        "crew_beep = data[\"isBeepCrew\"]\n",
        "\n",
        "move = data[\"move\"].tolist()\n",
        "\n",
        "\n",
        "print(\"Data size: \", len(data))\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "\n",
        "for i in range(len(data)-1):\n",
        "  a = ast.literal_eval(ship[i])\n",
        "  b = list(ast.literal_eval(alien_prob[i][1:-1]))\n",
        "  c = list(ast.literal_eval(crew_prob[i][1:-1]))\n",
        "  d = ast.literal_eval(bot[i])\n",
        "  e = [float(alien_beep[i])]\n",
        "  f = [float(crew_beep[i])]\n",
        "\n",
        "  X_train.append(a+b+c+d+e+f)\n",
        "  y_train.append(float(move[i]))\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "y_train_cat = to_categorical(y_train, num_classes=4)\n"
      ],
      "metadata": {
        "id": "9n0bjf7Ljo-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transformed = X_train[:,2500:5000]\n",
        "\n",
        "# Normalization\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data_transformed)\n",
        "\n",
        "# Dimensionality Reduction using auto-encoder\n",
        "input_layer = Input(shape=(data_scaled.shape[1],))\n",
        "encoded = Dense(256, activation='relu')(input_layer)\n",
        "encoded = Dropout(0.5)(encoded)\n",
        "encoded = Dense(128, activation='relu')(encoded)\n",
        "encoded = Dropout(0.5)(encoded)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "encoded = Dense(25, activation='relu')(encoded)\n",
        "\n",
        "decoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dropout(0.5)(decoded)\n",
        "decoded = Dense(128, activation='relu')(decoded)\n",
        "decoded = Dropout(0.5)(decoded)\n",
        "decoded = Dense(256, activation='relu')(decoded)\n",
        "decoded = Dense(data_scaled.shape[1], activation='sigmoid')(decoded)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(data_scaled, data_scaled, epochs=100, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "HrmCqZECjtRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing NaN values\n",
        "\n",
        "encoder = Model(input_layer, encoded)\n",
        "data_reduced = encoder.predict(data_scaled)\n",
        "df_belief_network_alien = pd.DataFrame(data_reduced)\n",
        "\n",
        "df_belief_network_alien = df_belief_network_alien.fillna(-999)\n",
        "for i in df_belief_network_alien:\n",
        "    if df_belief_network_alien[i].mean() < 0:\n",
        "        df_belief_network_alien = df_belief_network_alien.drop(columns = i)\n",
        "\n",
        "\n",
        "z_scores = np.abs(stats.zscore(df_belief_network_alien))\n",
        "df_belief_network_alien = z_scores\n",
        "\n",
        "df_belief_network_alien = df_belief_network_alien.fillna(-999)\n",
        "for i in df_belief_network_alien:\n",
        "    if df_belief_network_alien[i].mean() < 0:\n",
        "        df_belief_network_alien = df_belief_network_alien.drop(columns = i)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.plot(df_belief_network_alien)"
      ],
      "metadata": {
        "id": "z3b7Y__Gj2Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transformed = X_train[:,5000:7500]\n",
        "\n",
        "# Normalization\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data_transformed)\n",
        "\n",
        "# Dimensionality Reduction using auto-encoder\n",
        "input_layer = Input(shape=(data_scaled.shape[1],))\n",
        "encoded = Dense(256, activation='relu')(input_layer)\n",
        "encoded = Dropout(0.5)(encoded)\n",
        "encoded = Dense(128, activation='relu')(encoded)\n",
        "encoded = Dropout(0.5)(encoded)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "encoded = Dense(25, activation='relu')(encoded)\n",
        "\n",
        "decoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dropout(0.5)(decoded)\n",
        "decoded = Dense(128, activation='relu')(decoded)\n",
        "decoded = Dropout(0.5)(decoded)\n",
        "decoded = Dense(256, activation='relu')(decoded)\n",
        "decoded = Dense(data_scaled.shape[1], activation='sigmoid')(decoded)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(data_scaled, data_scaled, epochs=100, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "K6EFPve7kHXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing NaN values\n",
        "df_belief_network_crew = df_belief_network_crew.fillna(-1000)\n",
        "for i in df_belief_network_crew:\n",
        "    if df_belief_network_crew[i].mean() < 0:\n",
        "        df_belief_network_crew = df_belief_network_crew.drop(columns = i)\n",
        "\n",
        "z_scores = np.abs(stats.zscore(df_belief_network_crew))\n",
        "df_belief_network_crew = z_scores\n",
        "\n",
        "df_belief_network_crew = df_belief_network_crew.fillna(-1000)\n",
        "for i in df_belief_network_crew:\n",
        "    if df_belief_network_crew[i].mean() < 0:\n",
        "        df_belief_network_crew = df_belief_network_crew.drop(columns = i)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.plot(df_belief_network_crew)\n",
        "\n",
        "alien_enc = np.array(df_belief_network_alien)\n",
        "crew_enc = np.array(df_belief_network_crew)"
      ],
      "metadata": {
        "id": "tveFDu4kkNFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting legal moves\n",
        "X_tr_move = np.load(\"/content/drive/MyDrive/AI-Project-3/pos_moves_X_train.npy\")\n",
        "bpos_train = X_train[:,-4:-2]\n",
        "X_train_fin = np.concatenate((alien_enc,crew_enc,bpos_train,X_tr_move), axis = -1)"
      ],
      "metadata": {
        "id": "ZyXVI97gkXnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a dataset (X, y) and you want to split it into training and validation sets\n",
        "X_train_fin, X_test_fin, y_train_cat, y_test_cat = train_test_split(X_train_fin, y_train_cat, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_fin, y_train_cat, test_size=0.1, random_state=42)\n",
        "\n",
        "# Activation functions and their derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Softmax activation function with a mask\n",
        "def softmax(x, mask_in):\n",
        "    x_norm = x - np.max(x, axis=-1, keepdims=True)\n",
        "    exp_values = np.exp(x_norm)\n",
        "    exp_values = np.multiply(exp_values, mask_in)\n",
        "    sum_exp = np.sum(exp_values, axis=-1, keepdims=True) + 1e-5\n",
        "    probabilities = exp_values / sum_exp\n",
        "    return probabilities\n",
        "\n",
        "# Categorical crossentropy loss function\n",
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    epsilon = 1e-15\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    loss = -np.sum(y_true * np.log(y_pred)) / len(y_true)\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Initialize weights and biases\n",
        "input_size = X_train_fin.shape[1]\n",
        "hidden_size_1 = 64\n",
        "hidden_size_2 = 32\n",
        "hidden_size_3 = 16\n",
        "hidden_size_4 = 8\n",
        "hidden_size_5 = 4\n",
        "output_size = 4\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "weights_input_hidden_1 = np.random.rand(input_size, hidden_size_1)\n",
        "biases_hidden_1 = np.zeros((1, hidden_size_1))\n",
        "\n",
        "weights_hidden_1_hidden_2 = np.random.rand(hidden_size_1, hidden_size_2)\n",
        "biases_hidden_2 = np.zeros((1, hidden_size_2))\n",
        "\n",
        "weights_hidden_2_hidden_3 = np.random.rand(hidden_size_2, hidden_size_3)\n",
        "biases_hidden_3 = np.zeros((1, hidden_size_3))\n",
        "\n",
        "weights_hidden_3_hidden_4 = np.random.rand(hidden_size_3, hidden_size_4)\n",
        "biases_hidden_4 = np.zeros((1, hidden_size_4))\n",
        "\n",
        "weights_hidden_4_hidden_5 = np.random.rand(hidden_size_4, hidden_size_5)\n",
        "biases_hidden_5 = np.zeros((1, hidden_size_5))\n",
        "\n",
        "weights_hidden_5_output = np.random.rand(hidden_size_5, output_size)\n",
        "biases_output = np.zeros((1, output_size))\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "# Training loop with backpropagation for the provided architecture\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # Training\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        batch_X = X_train[i:i + batch_size]\n",
        "        batch_y = y_train[i:i + batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        hidden_layer_1_input = np.dot(batch_X, weights_input_hidden_1) + biases_hidden_1\n",
        "        hidden_layer_1_output = relu(hidden_layer_1_input)\n",
        "\n",
        "        hidden_layer_2_input = np.dot(hidden_layer_1_output, weights_hidden_1_hidden_2) + biases_hidden_2\n",
        "        hidden_layer_2_output = relu(hidden_layer_2_input)\n",
        "\n",
        "        hidden_layer_3_input = np.dot(hidden_layer_2_output, weights_hidden_2_hidden_3) + biases_hidden_3\n",
        "        hidden_layer_3_output = relu(hidden_layer_3_input)\n",
        "\n",
        "        hidden_layer_4_input = np.dot(hidden_layer_3_output, weights_hidden_3_hidden_4) + biases_hidden_4\n",
        "        hidden_layer_4_output = relu(hidden_layer_4_input)\n",
        "\n",
        "        hidden_layer_5_input = np.dot(hidden_layer_4_output, weights_hidden_4_hidden_5) + biases_hidden_5\n",
        "        hidden_layer_5_output = relu(hidden_layer_5_input)\n",
        "\n",
        "        output_layer_input = np.dot(hidden_layer_5_output, weights_hidden_5_output) + biases_output\n",
        "        output_layer_output = softmax(output_layer_input, batch_X[:,-4:])\n",
        "\n",
        "        # Compute loss\n",
        "        loss = categorical_crossentropy(batch_y, output_layer_output)\n",
        "        total_loss += loss\n",
        "\n",
        "        # Compute accuracy\n",
        "        predictions = np.argmax(output_layer_output, axis=1)\n",
        "        true_labels = np.argmax(batch_y, axis=1)\n",
        "        correct_predictions += np.sum(predictions == true_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        output_error = output_layer_output - batch_y\n",
        "        hidden_layer_5_error = np.dot(output_error, weights_hidden_5_output.T) * relu_derivative(hidden_layer_5_output)\n",
        "        hidden_layer_4_error = np.dot(hidden_layer_5_error, weights_hidden_4_hidden_5.T) * relu_derivative(hidden_layer_4_output)\n",
        "        hidden_layer_3_error = np.dot(hidden_layer_4_error, weights_hidden_3_hidden_4.T) * relu_derivative(hidden_layer_3_output)\n",
        "        hidden_layer_2_error = np.dot(hidden_layer_3_error, weights_hidden_2_hidden_3.T) * relu_derivative(hidden_layer_2_output)\n",
        "        hidden_layer_1_error = np.dot(hidden_layer_2_error, weights_hidden_1_hidden_2.T) * relu_derivative(hidden_layer_1_output)\n",
        "\n",
        "        # Update weights and biases using gradient descent\n",
        "        weights_hidden_5_output -= learning_rate * np.dot(hidden_layer_5_output.T, output_error)\n",
        "        biases_output -= learning_rate * np.sum(output_error, axis=0, keepdims=True)\n",
        "\n",
        "        weights_hidden_4_hidden_5 -= learning_rate * np.dot(hidden_layer_4_output.T, hidden_layer_5_error)\n",
        "        biases_hidden_5 -= learning_rate * np.sum(hidden_layer_5_error, axis=0, keepdims=True)\n",
        "\n",
        "        weights_hidden_3_hidden_4 -= learning_rate * np.dot(hidden_layer_3_output.T, hidden_layer_4_error)\n",
        "        biases_hidden_4 -= learning_rate * np.sum(hidden_layer_4_error, axis=0, keepdims=True)\n",
        "\n",
        "        weights_hidden_2_hidden_3 -= learning_rate * np.dot(hidden_layer_2_output.T, hidden_layer_3_error)\n",
        "        biases_hidden_3 -= learning_rate * np.sum(hidden_layer_3_error, axis=0, keepdims=True)\n",
        "\n",
        "        weights_hidden_1_hidden_2 -= learning_rate * np.dot(hidden_layer_1_output.T, hidden_layer_2_error)\n",
        "        biases_hidden_2 -= learning_rate * np.sum(hidden_layer_2_error, axis=0, keepdims=True)\n",
        "\n",
        "        weights_input_hidden_1 -= learning_rate * np.dot(batch_X.T, hidden_layer_1_error)\n",
        "        biases_hidden_1 -= learning_rate * np.sum(hidden_layer_1_error, axis=0, keepdims=True)\n",
        "\n",
        "    # Validation\n",
        "    val_total_loss = 0\n",
        "    val_correct_predictions = 0\n",
        "\n",
        "    for i in range(0, len(X_val), batch_size):\n",
        "        val_batch_X = X_val[i:i + batch_size]\n",
        "        val_batch_y = y_val[i:i + batch_size]\n",
        "\n",
        "        # Forward pass for validation\n",
        "        hidden_layer_1_input_val = np.dot(val_batch_X, weights_input_hidden_1) + biases_hidden_1\n",
        "        hidden_layer_1_output_val = relu(hidden_layer_1_input_val)\n",
        "\n",
        "        hidden_layer_2_input_val = np.dot(hidden_layer_1_output_val, weights_hidden_1_hidden_2) + biases_hidden_2\n",
        "        hidden_layer_2_output_val = relu(hidden_layer_2_input_val)\n",
        "\n",
        "        hidden_layer_3_input_val = np.dot(hidden_layer_2_output_val, weights_hidden_2_hidden_3) + biases_hidden_3\n",
        "        hidden_layer_3_output_val = relu(hidden_layer_3_input_val)\n",
        "\n",
        "        hidden_layer_4_input_val = np.dot(hidden_layer_3_output_val, weights_hidden_3_hidden_4) + biases_hidden_4\n",
        "        hidden_layer_4_output_val = relu(hidden_layer_4_input_val)\n",
        "\n",
        "        hidden_layer_5_input_val = np.dot(hidden_layer_4_output_val, weights_hidden_4_hidden_5) + biases_hidden_5\n",
        "        hidden_layer_5_output_val = relu(hidden_layer_5_input_val)\n",
        "\n",
        "        output_layer_input_val = np.dot(hidden_layer_5_output_val, weights_hidden_5_output) + biases_output\n",
        "        output_layer_output_val = softmax(output_layer_input_val, val_batch_X[:,-4:])\n",
        "\n",
        "        # Compute validation loss\n",
        "        val_loss = categorical_crossentropy(val_batch_y, output_layer_output_val)\n",
        "        val_total_loss += val_loss\n",
        "\n",
        "        # Compute validation accuracy\n",
        "        val_predictions = np.argmax(output_layer_output_val, axis=1)\n",
        "        val_true_labels = np.argmax(val_batch_y, axis=1)\n",
        "        val_correct_predictions += np.sum(val_predictions == val_true_labels)\n",
        "\n",
        "    # Calculate and print average training and validation loss and accuracy for every epoch\n",
        "    average_loss = total_loss / (len(X_train) / batch_size)\n",
        "    training_accuracy = correct_predictions / len(X_train)\n",
        "\n",
        "    val_average_loss = val_total_loss / (len(X_val) / batch_size)\n",
        "    val_accuracy = val_correct_predictions / len(X_val)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {average_loss}, Training Accuracy: {training_accuracy}, Validation Loss: {val_average_loss}, Validation Accuracy: {val_accuracy}\\n')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "hidden_layer_1_input_test = np.dot(X_test_fin, weights_input_hidden_1) + biases_hidden_1\n",
        "hidden_layer_1_output_test = relu(hidden_layer_1_input_test)\n",
        "\n",
        "hidden_layer_2_input_test = np.dot(hidden_layer_1_output_test, weights_hidden_1_hidden_2) + biases_hidden_2\n",
        "hidden_layer_2_output_test = relu(hidden_layer_2_input_test)\n",
        "\n",
        "hidden_layer_3_input_test = np.dot(hidden_layer_2_output_test, weights_hidden_2_hidden_3) + biases_hidden_3\n",
        "hidden_layer_3_output_test = relu(hidden_layer_3_input_test)\n",
        "\n",
        "hidden_layer_4_input_test = np.dot(hidden_layer_3_output_test, weights_hidden_3_hidden_4) + biases_hidden_4\n",
        "hidden_layer_4_output_test = relu(hidden_layer_4_input_test)\n",
        "\n",
        "hidden_layer_5_input_test = np.dot(hidden_layer_4_output_test, weights_hidden_4_hidden_5) + biases_hidden_5\n",
        "hidden_layer_5_output_test = relu(hidden_layer_5_input_test)\n",
        "\n",
        "output_layer_input_test = np.dot(hidden_layer_5_output_test, weights_hidden_5_output) + biases_output\n",
        "output_layer_output_test = softmax(output_layer_input_test, X_test_fin[:,-4:])\n",
        "\n",
        "test_loss = categorical_crossentropy(y_test_cat, output_layer_output_test)\n",
        "\n",
        "# Calculate and print test accuracy\n",
        "test_predictions = np.argmax(output_layer_output_test, axis=1)\n",
        "test_true_labels = np.argmax(y_test_cat, axis=1)\n",
        "test_accuracy = np.sum(test_predictions == test_true_labels) / len(X_test_fin)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "id": "f91JQ0FXkdze"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}